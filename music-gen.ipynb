{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"hparams = {\n    \"n_layer\": 4,\n    \"n_head\": 4,\n    \"embedding_dim\": 32,\n    \"max_seq_length\": 2048,\n    \"lr\": 1e-4,\n    \"batch_size\": 8,\n    \"epoch_length\": 100,\n    \"epochs\": 100,\n}\nhparams[\"iters\"] = hparams[\"epochs\"] * hparams[\"epoch_length\"]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport requests\nimport math\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport json\nimport transformers\nfrom tqdm.notebook import tqdm\n\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"r = requests.get('https://raw.githubusercontent.com/adactio/TheSession-data/master/json/tunes.json')\nassert r.status_code == 200\ndata = pd.DataFrame(json.loads(r.text))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Strip all whitespaces from abc\ndata['abc'] = data['abc'].map(lambda text: text.replace(\" \", \"\").replace(\"\\r\", \"\").replace(\"\\n\", \"\").replace(\"\\t\", \"\").replace(\"\\x14\", \"\").replace(\"\\x1a\", \"\").replace(\"\\xa0\", \"\").replace(u\"\\u2028\", \"\"))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dictionary = list(data['abc'].map(lambda text: [char for char in text]))\ndictionary = [item for sublist in dictionary for item in sublist]\ndictionary = ['<bos>', '<eos>'] + list(np.unique(dictionary))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def tensor_to_abc(data):\n    return ''.join([dictionary[x] for x in data if x != -100 and x != 0 and x != 1])\n\ndef abc_to_list(text):\n    return [dictionary.index('<bos>')] + [dictionary.index(char) for char in text] + [dictionary.index('<eos>')]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"features = data['abc'].map(lambda text:  abc_to_list(text))\nfeatures = [x for x in features if len(x) <= hparams['max_seq_length']]\nfeatures.sort(reverse=True, key=lambda x: len(x))\nfeatures = torch.nn.utils.rnn.pad_sequence([torch.tensor(x) for x in features], batch_first=True, padding_value=-100)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"features.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = transformers.GPT2LMHeadModel(transformers.GPT2Config(\n    vocab_size = len(dictionary),\n    n_embd = hparams[\"embedding_dim\"],\n    n_layer = hparams[\"n_layer\"],\n    n_head = hparams[\"n_head\"],\n    n_positions = hparams['max_seq_length'],\n    n_ctx = hparams['max_seq_length']\n))\noptim = torch.optim.Adam(model.parameters(), lr=hparams[\"lr\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\nmodel.to(device)\nfeatures = features.to(device) # The entire dataset fits on a gpu easily","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.train()\nfor i in tqdm(range(0, hparams['iters'])):\n    indices = np.random.choice(len(features), hparams['batch_size'])\n    batch = features[indices].clone()\n    mask = batch == -100\n    batch[mask] = 0\n    \n    optim.zero_grad()\n    loss, predictions, past = model.forward(batch, attention_mask=mask, labels=features[indices])\n    loss.backward()\n    optim.step()\n    \n    if i % hparams['epoch_length'] == 0:\n        tqdm.write('Epoch %d, loss %f' % (i // hparams['epoch_length'], loss.cpu()))\n        torch.save(model.state_dict(), '/kaggle/working/model.pth')\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def synthesize(starting_sequence):\n    model.eval()\n    with torch.no_grad():\n        starting_sequence_tensor = torch.tensor(abc_to_list(starting_sequence)).unsqueeze(0).to(device)\n        pred, _ = model.forward(starting_sequence_tensor)\n        pred = pred.cpu()[0]\n        return starting_sequence + tensor_to_abc([x.argmax() for x in pred])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"synthesize(\"aabBB\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}